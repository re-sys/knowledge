- 回顾
	- query 对每一个keys获得一个注意力权重，之后做加权和，看与哪个key最相关即可
- 扩展到高维度
	- 假设query 是k维度，实际就是将softmax中的权重为a函数
- 常见a函数
	- additive attention
		- $a(k,q)=v^{T}\tanh(W_{k}k+W_{q}q)$
		- 可学参数 $W_{k},W_{q},v$
		- 等价与将key和query都做一个单隐藏层加权，最后做内积获得值
	- scaled Dot-Product Attention
		- 如果query 和key都是同样长度，那么可以
			- $a(Q,K)= QK^{T}/\sqrt{ d }$
		- 向量化版本就是
			- $a(Q,K)= QK^{T}/\sqrt{ d }$
- 小结
	- 注意力分数是query 和key的相似度，注意力权重是分数的softmax的结果
	- 两种常见计算
		- 将query,key合并起来今已一个单输出层MLP
		- 直接将query和key做内积
- 问题 
	- 实际就是把先前的x,q改为x_i,k_i
	- 注意力分数一般使用相似度来衡量，比如调查自己的工资，一般就与自己相似的做调查
	- masked_softmax我偷偷告诉你有些东西确实没用，不管多相似都不用
- 下一个[[注意力机制seq2seq]]