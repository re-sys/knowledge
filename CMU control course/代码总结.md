- QP
	- 对于QP进行迭代，就是一般都是convex的function，不需要做regularization
	- 使用如果使用增广拉格朗日，必须将等式约束的也加上，否则lam无法使用梯度上升公式更新
	- 同样对merit函数必须是完整的cost function，不然无法完成正常知道自己的步长
	- 建议做line search，有的时候需要，有的时候不用，c=1e-4就是比上一次要足够小
- LQR
	- 就是求一个最优控制的问题，将N个状态和u进行二次项拟合，最后使用每次算出的最好的u进行控制
	- 对于Q R Qf矩阵本身的值是不用在意的，他们只要关心相互之间的比例问题,比如最终的Qf=5*Q
	- 求解方式可以不用凸优化（需要求大矩阵，还是有较高复杂度）
	- 可以使用Riccati recursion求解，迭代P矩阵 和K gain矩阵
	- 之后控制的时候使用u = -$K[i]*(x-x_{goal})$就可以了,这样做让有噪声的系统也可以正常工作，即使有x累积误差，也可以通过当前的x进行直接修正，保证负反馈控制
	- 对于通过迭代计算后最开始的K会逼近一个常数，这个就是infinite-horiaon LQR gain 他会满足我们最优解的条件
	- 对于不可控的系统，这个凸优化问题肯定是有解的，但是可能就不能保证一定可以控制到位，还有这个方法只依赖模型，不会依赖初始值。所以有很好的泛化性，其实我们只需要最开始的K可能就足以完成全部控制，只是接近目标后可能不会那么稳定。
	- 更新公式
	- `S = R + B'*P[k+1]*B` `K[k] = (S\(B'*P[k+1]*A))` `P[k] = Q + A'*P[k+1]*A - A'*P[k+1]*B*K[k]`